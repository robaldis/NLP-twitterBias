{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formating the data\n",
    "The goal for this notebook is to format the data into something that is useable to clean up the data.\n",
    "\n",
    "### Imports\n",
    "- tweepy - The data object that the data is saved in\n",
    "- pickle - To unpickle the variables that have been saved\n",
    "- pandas - To reprosent the data in a dataframe\n",
    "- numpy - Working with arrays\n",
    "- glob - Easy way to gather all the files in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pickle\n",
    "import pandas\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the data\n",
    "\n",
    "This is just to understand that data in each file and how to index it.\n",
    "to get the information that we want we need to index the key firstly, then there will be a list of tweet data objects. To get the text of the tweet its then just ```.text```\n",
    "\n",
    "The data comes in as such:\n",
    "```{\"user\" : [{tweet}, {tweet}, {tweet}]}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = \"12.p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open (file, \"rb\") as f:\n",
    "#    data = pickle.load(f)\n",
    "#    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the data\n",
    "\n",
    "This chunk of code collects all the names of the files that need to be opened so they can be user with the ```open()``` function to read all of the data in each file. \n",
    "\n",
    "The next chunk of code is similar to above where it collects all of the informaiont of the users tweets and adds them to a dictionary with an array of users and an array of users 1st, 2nd, ..., 1000th tweet. This seemed to be the easiest way to format the data to work with pandas data frame and show the data cohesivley. The data also needs to have the same amount of tweets for each user so we have to check if they have enough tweets before we start putting thier informaion in the data.\n",
    "\n",
    "The last part of the code trys to append the tweet text to the tweet array, however, if it is the first tweet to go in then it has to create that array so the exception does so.\n",
    "\n",
    "Finally we can add this dictionary to the data frame and show it in a nice looking table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for name in glob.glob('../tweets/*'):\n",
    "    print (name)\n",
    "    files.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# userData = {\n",
    "#     \"User\": np.array([]),\n",
    "#     \"tweets\": np.array([])\n",
    "# }\n",
    "# for j in range (50):\n",
    "#     file = files[j]\n",
    "#     with open(file, 'rb') as f:\n",
    "#         accounts = pickle.load(f)\n",
    "#         for user, value in accounts.items():\n",
    "#             combTweet = \"\"\n",
    "#             if (len(value) < 1000):\n",
    "#                 break\n",
    "#             userData[\"User\"]=np.append(userData[\"User\"], [user])\n",
    "#             for i in range(1000):\n",
    "#                 tweet = value[i]\n",
    "#                 combTweet = combTweet+ tweet.text\n",
    "                \n",
    "#             userData[\"tweets\"] = np.append(userData['tweets'], [combTweet])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making it more efficient\n",
    "To make this more efficient on memory this code is going to have to be executed in batches and saved out to a file. I can do it a format like CSV or in jason form. For this task I think i'll use CSV.\n",
    "\n",
    "There is an error at the moment where the last batch will crash as the index goes out of bounds of the array of files. This doesn't break everything just stops at the last batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToFile(data,file):\n",
    "    with open (file, \"a+\", encoding=\"utf-8\") as f:\n",
    "        for i in range(len(data[\"User\"])):\n",
    "            key = data[\"User\"][i]\n",
    "            value = data[\"tweets\"][i]\n",
    "            f.write(f\">{key},{value}\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 0\n",
    "batchSize = 5\n",
    "while True:\n",
    "    userData = {\n",
    "        \"User\": np.array([]),\n",
    "        \"tweets\": np.array([])\n",
    "    }\n",
    "    for j in range (batchSize):\n",
    "        if (j + (iter * batchSize) > len(files)):\n",
    "            break\n",
    "        file = files[j + (iter * batchSize)]\n",
    "        with open(file, 'rb') as f:\n",
    "            accounts = pickle.load(f)\n",
    "            for user, tweets in accounts.items():\n",
    "                combTweet = \"\"\n",
    "                # does that user have more than 1000 tweets, if not move on to the next user\n",
    "                if (len(tweets) < 1000):\n",
    "                    break\n",
    "                # add the user to the dictionary\n",
    "                userData[\"User\"]=np.append(userData[\"User\"], [user])\n",
    "                # gather all of the tweets and add them to the dictionary\n",
    "                for i in range(1000):\n",
    "                    # is the tweet a retweet or not\n",
    "                    if (hasattr(tweets[i], \"retweeted_status\")):\n",
    "                        break\n",
    "                    tweet = tweets[i]\n",
    "                    # Get the full text of the tweet without truncation\n",
    "                    combTweet = combTweet + tweet.full_text.strip().replace('\\n', '').replace(',', ' ')\n",
    "                userData[\"tweets\"] = np.append(userData['tweets'], [combTweet])\n",
    "    # Increase the iterations to load more files\n",
    "    iter += 1\n",
    "    # Save out to a file\n",
    "    saveToFile(userData, \"userData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame(userData)\n",
    "df.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open ('pickle/rawDataFrame.p', 'wb') as f:\n",
    "    pickle.dump(df,f)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}